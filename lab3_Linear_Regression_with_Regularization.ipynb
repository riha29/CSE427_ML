{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**\n",
        "\n",
        "Let's say, we want to predict house rent based on three features.If $x$ is a house, then\n",
        "\n",
        "> $x_{1}$ = total square feet of that house\n",
        "\n",
        "> $x_{2}$ = total number of rooms\n",
        "\n",
        "> $x_{3}$ = number of washrooms\n",
        "\n",
        "Actually there are other important variables to determine the price of a household. But for simplicity, let's stick to these 3. In linear regression, we assume that the rent is a linear function of $x_{1}, x_{2}, x_{3}$. Let's call the rent of this house $y$. So the following equation holds:\n",
        "\n",
        ">> $h_{w}(x) = w_{1}x_{1} + w_{2}x_{2} + .... w_{n}x_{n} + b$\n",
        "\n",
        "Here, $w_{1}, w_{2}, ... $ are constants that are called weights and $b$ is a constant that is called the bias. In linear regression, we learn these weights and biases from the dataset.\n",
        "\n",
        "Let's say in our dataset, we have $m$ houses $x_{1}, x_{2}, .... x_{m}$ and their respective rents $y_{1}, y_{2}, ....., y_{m}$.\n",
        "\n",
        "And let's say, there are $n$ features of each house. So, for house number $i$, the features can be accessed as $x_{i, 1}, x_{i, 2}, ... x_{i, n}$. Therefore our prediction of the price would be:\n",
        "\n",
        ">> $h_{w}(x_{i}) = w_{1}x_{i, 1} + w_{2}x_{i, 2} + .... w_{n}x_{i, n} + b$\n",
        "\n",
        "So, how much is the error? That's $y_{i} - h_{w}(x_{i})$. But in Machine Learning, we use the squared error because of it's convex properties. So, we will consider the error to be $(y_{i} - h_{w}(x_{i}))^{2}$.\n",
        "\n",
        "But that's just the error for house $i$. The total error for all $m$ houses is:\n",
        "\n",
        ">> $E = \\sum_{i=1}^{m} (y_{i} - h_{w}(x_{i}))^{2}$\n",
        "\n",
        "\n",
        "We want to find such values of $w_{1}, w_{2}, .... b$ so that the above error is minimized. Since this error function is convex, we use gradient descent.\n",
        "\n",
        "The weight updates are done in the following way:\n",
        "\n",
        "$w_{1} = w_{1} - \\lambda \\frac{\\delta E}{\\delta w_{1}}$\n",
        "\n",
        "$w_{2} = w_{2} - \\lambda \\frac{\\delta E}{\\delta w_{2}}$\n",
        "\n",
        "....\n",
        "\n",
        "$w_{n} = w_{n} - \\lambda \\frac{\\delta E}{\\delta w_{n}}$\n",
        "\n",
        "$b = b - \\lambda \\frac{\\delta E}{\\delta b}$\n",
        "\n",
        "Where $\\lambda$ is a constant.\n",
        "\n",
        "But what is the value of $\\frac{\\delta E}{\\delta w_{1}}$? For any $j$, the value can be determined using simple differentiation and the final formula is:\n",
        "\n",
        ">> $\\frac{\\delta E}{\\delta w_{j}} = \\sum_{i = 1}^{m} 2 (y_{i} - h_{w}(x_{i}))(-x_{i, j})$\n",
        "\n",
        "and for $b$ the formula is:\n",
        "\n",
        ">> $\\frac{\\delta E}{\\delta b} = \\sum_{i = 1}^{m} 2 (y_{i} - h_{w}(x_{i}))(-1)$"
      ],
      "metadata": {
        "id": "0kbA7vDtWr9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression Pseudocode**\n",
        "This is a skeleton of linear regression with gradient descent\n",
        "\n",
        "initialize values of $w_{1}, w_{2}, w_{3},...., b$ with random values\n",
        "\n",
        "iterations = let's say 10000\n",
        "\n",
        "lambda = 0.1\n",
        "\n",
        "for iter in range(iterations):\n",
        "> For each house $x_{i}$, predict the house rent $h_{w}(x_{i})$ using the current weights and biases\n",
        "\n",
        "> update the weights using the above formulae"
      ],
      "metadata": {
        "id": "_kxSVqdNbx5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization and Weight Decay**\n",
        "In linear regression our objective function is the mean square error or the sum of squared errors. We want to minimize that.\n",
        ">> $E = \\sum_{i = 1}^{m} (y_{i} - h_{w}(x_{i}))^{2}$\n",
        "\n",
        "But, a complex hypothesis might also minimize this empirical error. And we might pick this hypothesis over a simpler one. But according to **Occam's Razor**, we must choose a simpler hypothesis over a complex one. One way to define simplicity is to have a smaller norm. This way we are biasing the weight vector to have a smaller norm. But from the **No Free Lunch Theorem** we know that, learning algorithms perform better with a bit of inductive bias. And also reduces the variance of the solution. So, in order to penalize more complex hypothesis we can minimize the following objective function.\n",
        "\n",
        ">> $ E = \\sum_{i = 1}^{m} (y_{i} - h_{w}(x_{i}))^{2} + \\alpha$ $c(h)$\n",
        "\n",
        "Where $c(h)$ means the complexity of the hypothesis $h$. There are many ways of defining the complexity of a hypothesis. Two of the most popular ways is to take:\n",
        "\n",
        "$c(h) = |w_{1}|^{2} + |w_{2}|^{2} + \\cdots + |w_{n}|^{2}$, here we have taken $c(h)$ to be the $L_{2}$ norm of the weight vector. This form of linear regression is called **Ridge Regression**. or\n",
        "\n",
        "\n",
        "$c(h) = |w_{1}| + |w_{2}| + \\cdots + |w_{n}|$, here we have taken $c(h)$ to be the $L_{1}$ norm of the weight vector. This form of linear regression is called **Lasso Regression**.\n",
        "\n",
        "where $w_{i}$ are the parameters of the hypothesis and $\\alpha$ is a constant which is called the weight decay parameter. This sort fo regularization is called '**weight decay**'.\n"
      ],
      "metadata": {
        "id": "bJjYEmeUanAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synthetic Data Generation**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E4rmT5T8yvol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "FeqnsCmzx7cR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 100*x[2] - 17*x[1] + 3*x[0] + 11"
      ],
      "metadata": {
        "id": "Bi37McudxFSo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(a, b, mean, std, shape):\n",
        "  x = np.random.uniform(a, b, shape) # takes 'size' number of random values from the range [a, b]\n",
        "  y = [f(a) for a in x] # for each x, calculates f(x)\n",
        "  noise = np.random.normal(mean, std, shape[0]) # for each x, calculates some noise\n",
        "  y = y + noise # adds the noise to f(x)\n",
        "  y = np.array(y)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "gt89Psaqwmbx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_size = 1000\n",
        "features = 3\n",
        "train_x, train_y = generate(-1, 1, 0, 0.1, (training_size, features))"
      ],
      "metadata": {
        "id": "WC5FnM9OwsZn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Task**\n",
        "> Here 1000 training data has been generated. Each $x$ has 3 features.\n",
        "\n",
        "> There is an underlying function $f(x) = 100x_{2} - 17x_{1} + 3x_{0} + 11 + \\epsilon$ which was used to generate the data.\n",
        "\n",
        "> Ïµ is noise that was chosen from a normal distribution of mean 0 and standard deviation of 0.1\n",
        "\n",
        "> But the student should pretend that they don't know this function. Instead they should try to use linear regresion to find the values of the weights and biases in the following function\n",
        "\n",
        ">>> $h_{w}(x) = w_{2}x_{2} + w_{1}x_{1} + w_{0}x_{0} + b$\n",
        "\n",
        "> And see if these coefficients are close to 100, -17, 3 and 11.\n",
        "\n",
        "> In order to do so, they must use 3 types of linear regression.\n",
        "\n",
        "\n",
        "\n",
        "1.   Plain linear regression\n",
        "2.   Lasso linear regression\n",
        "3.   Ridge linear regression\n",
        "\n",
        "> In all of them, the weight update rule is the same. That is:\n",
        "\n",
        ">> $w_{j} = w_{j} - \\lambda \\frac{\\delta E}{\\delta w_{j}}$ for all $1 \\leq j \\leq n$\n",
        "\n",
        ">> $b = b - \\lambda \\frac{\\delta E}{\\delta b}$\n",
        "\n",
        "> But depending on the type of the regression, the definition of $E$ varies. The student himself/herself should find the formula for $\\frac{\\delta E}{\\delta w_{j}}$ in each case.\n",
        "\n",
        "> In the case of plain linear regression, the formulae are given above.\n",
        "\n",
        "> For this lab, take $\\lambda = 0.1$, $\\alpha = 0.01$, number of iterations = 10000.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZG8Nz9isyq8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plain linear regression\n",
        "def plain_linear_regression(X, y, l_rate, iterations):\n",
        "  m, n = X.shape\n",
        "  w = np.zeros(n)\n",
        "  b = 0\n",
        "  for i in range(iterations):\n",
        "    y_pred = np.dot(X, w) + b\n",
        "    dw = -2/m * np.dot(X.T, (y - y_pred))\n",
        "    db = -2/m * np.sum(y - y_pred)\n",
        "    w -= l_rate * dw\n",
        "    b -= l_rate * db\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "ic4Z7A6Kr52i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lasso linear regression\n",
        "def lasso_linear_regression(X, y, l_rate, alpha, iterations):\n",
        "  m, n = X.shape\n",
        "  w = np.zeros(n)\n",
        "  b = 0\n",
        "  for i in range(iterations):\n",
        "    y_pred = np.dot(X, w) + b\n",
        "    dw = -2/m * np.dot(X.T, (y - y_pred)) + alpha * np.sign(w)\n",
        "    db = -2/m * np.sum(y - y_pred)\n",
        "    w -= l_rate * dw\n",
        "    b -= l_rate * db\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "ej60qIAZp560"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ridge linear regerssion\n",
        "def ridge_linear_regression(X, y, l_rate, alpha, iterations):\n",
        "  m, n = X.shape\n",
        "  w = np.zeros(n)\n",
        "  b = 0\n",
        "  for _ in range(iterations):\n",
        "    y_pred = np.dot(X, w) + b\n",
        "    dw = -2/m * np.dot(X.T, (y - y_pred)) + 2 * alpha * w\n",
        "    db = -2/m * np.sum(y - y_pred)\n",
        "    w -= l_rate * dw\n",
        "    b -= l_rate * db\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "r4bbVBSsqH1o"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}